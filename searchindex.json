{"categories":[],"posts":[{"content":" 前景 Ceph 目前自带了一些单机存储引擎，包括\n 基于文件的FileStore; 基于KV的KStore； 基于RocksDB+裸盘的BlueStore； 基于内存的测试用MemStore； 基于seastar实现的全用户态SeaStore。  这些单机存储引擎都有适用的用户场景和读写模型。对于海量小文件的场景下，KStore 是一个很好的选择，LSM 架构的 KV 引擎天然的将数据聚合为大文件，顺序的存储到磁盘中。但是，KStore 一直以来应用较少，社区在上面维护的资源也较小。\n最近，我们希望在 Rocksdb 层面进行优化，让 Rocksdb 对 blob 写入进行优化。从而使得 KStore 不仅可以支持海量小文件，也可以较好的支持大文件。\n让 RocksDB 支持 Blob 存储 原生的 RocksDB 其实不适合存储Blob（Value 较大），因为分层模型和日志的存在，数据放大会更明显，KV在执行后台compaction也会更慢。业界提出的解决方案主要就是“Key Value 分开存储”，这样可以独立对 Blob 文件进行优化，减少写放大和compaction影响。具体可以阅读 Wisckey 的论文（阿里云应该在13年的时候就已经在内部KV实现了这类优化，支持其上OSS类BLOB业务)。\n目前，PingCAP 公司基于 RocksDB 和 Wisckey 实现了一个 RocksDB 的插件 TitanDB. 上层程序可以简单的通过 Titan::DB 的命名空间来使用。因此，我们将Titan 替换原生的 RocksDB，使得 KStore 可以较好的存储大文件。\n理论上，KStore 层面的代码完全不用修改，性能也不会有损耗。相对于 FileStore，没有 Double Write 的过程，性能更好。相对于 BlueStore，实现上更简单更易维护，对小文件更友善，性能类似。具体的性能报告待整理完善。\nRead Stripe Cache 导致的内存泄漏  issue地址 PR地址 与社区讨论地址  这个问题在19年5月分的时候就已经提交到社区，中间因为同事转岗以及社区对read-cache是否有意义有分歧，拖到最近才合并到主线。\n简单的讲，问题是KStore会为一个 transaction 的写入读取缓存 stripe，来保护一个事务中的写入只用更新内存。这块Cache会在事务结束的时候释放。但是在读文件的时候也有类似的Cache，但是读操作并不通过事务提交，这块儿内存没有释放。本质上读类操作并不需要 stripe cache，经过来来回回的讨论，大家终于达成一致，彻底在 read 操作中移除 stripe cache。\nOnode Cache 导致的内存泄漏  issue地址 PR地址  在KStore的实现中，每打开一个 RADOS obj都会在内存中缓存 onode 结构体。在长时间的测量中发现，内存占用会缓慢上涨，直到 OOM。通过代码阅读，怀疑是 Onode Cache 没有释放。直接 GDB 线上进程，可以发现每个 collection 缓存了 9k+ 的 Onode, 当前进程共打开了399个 collection。通过计算，这一块就占用了1.7GB的内存。\n为了更好的观察 onode 的内存占用，我们添加了 mempool 对 KStore 的支持，可以直接通过 asock 命令来查看实时的 onode 个数与内存占用，对应的代码为链接。\n查看了 BlueStore 的实现，会有一个后台线程 MempoolThread 周期的释放 Onode Cache，同时也会在读写操作时，根据 Cache 来释放一些 Onode Cache。目前，我们简单的添加了一个后台进程来周期的释放 Onode Cache，将来可以基于 mempool 来做更精细化的内存管控。\nPending Stripes 并发访问的 SegFault Core  issue地址 PR地址  这个问题比较简单，在测试的时候遇到 OSD Core，通过 GDB 怀疑和 pending stripes 结构体的并发访问有关。加锁验证了一下，确实不会再出现 SegFault。于是修改为性能更好的读写锁。\n","id":0,"section":"posts","summary":"前景 Ceph 目前自带了一些单机存储引擎，包括 基于文件的FileStore; 基于KV的KStore； 基于RocksDB+裸盘的BlueStore； 基","tags":null,"title":"最近在 Ceph KStore 上的一些工作（2020-01-13）","uri":"https://liuchang0812.github.io/2020/01/recent-works-on-ceph-kstore/","year":"2020"},{"content":"一句话总结就是：每个会话在指定目录下创建一个键值对，谁先创建成功谁就是主。\n背景 我们在使用 etcd 时，会利用 etcd 来实现进程抢主的逻辑。 etcd go 的SDK提供了封装好的 concurrency 模块来简化分布式选主的实现。如下，是一个忽略了错误处理的选主实现示例。\n 那么，etcd clientv3 的 Election 具体是怎么实现的呢？如何保证在任意情况下只有一个实例成为 leader ？\n内部实现 etcd 维护了一个全局递增的版本号，对应于每次原子修改。这个版本号称之为 Version。相应的，对于每个保存在 etcd 中的 Key 也保存了这个 Key 是在哪一个 Version 时创建的信息，称之为 CreateRevision。\n当用户调用Election.Campaign方法时，etcd client 就会尝试创建一个 Key(如何已经存在，就不创建，直接取值)，并纪录下这个 Key 的 CreateRevision 。接着，etcd client会遍历指定目录下所有的版本小于 CreateRevision 的 Key。如果不存在小于 CreateRevision 的 Key，则说明当前创建的 Key 就是最早的 Key 了，该会话可以安全的成为 leader。否则，etcd client 会一直监听目录下这些小于 CreateRevision 的 Key，直到所有的 Key 都被删除。\n","id":1,"section":"posts","summary":"\u003cp\u003e一句话总结就是：每个会话在指定目录下创建一个键值对，谁先创建成功谁就是主。\u003c/p\u003e","tags":null,"title":"Etcd SDK 中的选主实现","uri":"https://liuchang0812.github.io/2019/11/etcd-client-election/","year":"2019"},{"content":" 为什么要发明 RAFT 一致性协议？ 因为 paxos 过于复杂，难以理解。同时，paxos 是面向学术和理论证明，没有在论文中介绍工程上的实现。如果要实现一个工业级的 paxos，要做很多优化拓展，例如 multi-paxos。为了解决这些问题， raft 是一个把可理解性、明确工程实现方法放在首要考虑地位的算法。\nRAFT 算法细节 raft 算法主要分为两个部分: 1)Leader Election; 2)Log Replication。\n基本概念  term, 每次 leader 变更时就增长1。每个 term 内只会有一个 leader。时间的逻辑划分，用于 leader election。 RPC，不同结点之间通过 RPC 来通信，在 raft 中有两个 RPC: 1)RequestVote;2)AppendEntries。  Leader Election 空的 AppendEntries RPC 被用来作为心跳包，由 Leader 发送给其它 Server，同时维持其的Leader角色（Leader Authority）。当一个 Server 在超过一定时间(election timeout)没有收来自Leader的心跳包，就认为 Leader 不存在，开始新一轮的选举。\n选举的策略比较简单，每个 Server 在每个 Term 只给一个人投票，遵循先到先得的策略。为了尽量避免出现 split 投不出大多数的情况，每个 Server 都有一个随机的election timeout配置，来避免多个 Server 在同时发出 RequestVote 请求。\nLog Replication committed log: 保证持久化的日志，不会因为机器故障等各类原因丢失该日志。并且该日志最终会被所有可用机器执行。\n日志满足一个特性 Log Matching Property，类似于归纳法。对于不同的日志，只有他们的term和index相同,他们的内容一定相同。同时他们之前日志的内容也相同。\nConsistency Check: When send- ing an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries. If the follower does not find an entry in its log with the same index and term, then it refuses the new entries.\nLeader 日志是immutable的，follower 的日志是可以覆盖的：\na leader creates at most one entry with a given log index in a given term, and log entries never change their position in the log. the leader handles inconsistencies by forcing the followers’ logs to duplicate its own. This means that conflicting entries in follower logs will be overwritten with entries from the leader’s log.\nLeader 通过给 Follower 不停的发送 AppendEntries RPC 来找到 follower 的日志起点（term和index都相同）。\nSafety Leader Completeness Property: the leader for any given term con- tains all of the entries committed in previous terms Leader Completeness Property: Leader Completeness Prop\n","id":2,"section":"posts","summary":"为什么要发明 RAFT 一致性协议？ 因为 paxos 过于复杂，难以理解。同时，paxos 是面向学术和理论证明，没有在论文中介绍工程上的实现。如果要实现一个工业级","tags":null,"title":"Raft Paper Notes","uri":"https://liuchang0812.github.io/2019/10/raft-paper-notes/","year":"2019"},{"content":" Build Seastar git clone https://github.com/scylladb/seastar.git cd seastar git submodule update --init --recursive apt udpate \u0026amp;\u0026amp; ./install-dependencies.sh ./configure.py --mode=release --cook fmt  Link with Seastar apt install libunistring-dev export seastar_dir=`pwd` g++ my_app.cc $(pkg-config --libs --cflags --static $seastar_dir/build/release/seastar.pc) -o my_app ./my_app  ","id":3,"section":"posts","summary":" Build Seastar git clone https://github.com/scylladb/seastar.git cd seastar git submodule update --init --recursive apt udpate \u0026amp;\u0026amp; ./install-dependencies.sh ./configure.py --mode=release --cook fmt  Link with Seastar apt install libunistring-dev export seastar_dir=`pwd` g++ my_app.cc $(pkg-config --libs --cflags --static $seastar_dir/build/release/seastar.pc) -o my_app ./my_app  ","tags":null,"title":"Build and Link with SeaStar library","uri":"https://liuchang0812.github.io/2019/09/install-seastar/","year":"2019"},{"content":"","id":4,"section":"posts","summary":"","tags":null,"title":"My First Post","uri":"https://liuchang0812.github.io/2019/01/my-first-post/","year":"2019"}],"tags":[]}