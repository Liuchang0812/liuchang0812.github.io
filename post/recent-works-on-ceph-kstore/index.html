<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Notes  | 最近在 Ceph KStore 上的一些工作（2020-01-13）</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.68.3" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://liuchang0812.github.io/dist/css/app.955516233bcafa4d2a1c13cea63c7b50.css" rel="stylesheet">
    

    

    
      
    

    

    <meta property="og:title" content="最近在 Ceph KStore 上的一些工作（2020-01-13）" />
<meta property="og:description" content="背景 Ceph 目前自带了一些单机存储引擎，包括
 基于文件的FileStore; 基于KV的KStore； 基于RocksDB&#43;裸盘的BlueStore； 基于内存的测试用MemStore； 基于seastar实现的全用户态SeaStore。  这些单机存储引擎都有适用的用户场景和读写模型。对于海量小文件的场景下，KStore 是一个很好的选择，LSM 架构的 KV 引擎天然的将数据聚合为大文件，顺序的存储到磁盘中。但是，KStore 一直以来应用较少，社区在上面维护的资源也较小。
最近，我们希望在 Rocksdb 层面进行优化，让 Rocksdb 对 blob 写入进行优化。从而使得 KStore 不仅可以支持海量小文件，也可以较好的支持大文件。
让 RocksDB 支持 Blob 存储 原生的 RocksDB 其实不适合存储Blob（Value 较大），因为分层模型和日志的存在，数据放大会更明显，KV在执行后台compaction也会更慢。业界提出的解决方案主要就是“Key Value 分开存储”，这样可以独立对 Blob 文件进行优化，减少写放大和compaction影响。具体可以阅读 Wisckey 的论文（阿里云应该在13年的时候就已经在内部KV实现了这类优化，支持其上OSS类BLOB业务)。
目前，PingCAP 公司基于 RocksDB 和 Wisckey 实现了一个 RocksDB 的插件 TitanDB. 上层程序可以简单的通过 Titan::DB 的命名空间来使用。因此，我们将Titan 替换原生的 RocksDB，使得 KStore 可以较好的存储大文件。
理论上，KStore 层面的代码完全不用修改，性能也不会有损耗。相对于 FileStore，没有 Double Write 的过程，性能更好。相对于 BlueStore，实现上更简单更易维护，对小文件更友善，性能类似。具体的性能报告待整理完善。
Read Stripe Cache 导致的内存泄漏  issue地址 PR地址 与社区讨论地址  这个问题在19年5月分的时候就已经提交到社区，中间因为同事转岗以及社区对read-cache是否有意义有分歧，拖到最近才合并到主线。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liuchang0812.github.io/post/recent-works-on-ceph-kstore/" />
<meta property="article:published_time" content="2020-01-13T15:50:31+08:00" />
<meta property="article:modified_time" content="2020-01-13T15:50:31+08:00" />
<meta itemprop="name" content="最近在 Ceph KStore 上的一些工作（2020-01-13）">
<meta itemprop="description" content="背景 Ceph 目前自带了一些单机存储引擎，包括
 基于文件的FileStore; 基于KV的KStore； 基于RocksDB&#43;裸盘的BlueStore； 基于内存的测试用MemStore； 基于seastar实现的全用户态SeaStore。  这些单机存储引擎都有适用的用户场景和读写模型。对于海量小文件的场景下，KStore 是一个很好的选择，LSM 架构的 KV 引擎天然的将数据聚合为大文件，顺序的存储到磁盘中。但是，KStore 一直以来应用较少，社区在上面维护的资源也较小。
最近，我们希望在 Rocksdb 层面进行优化，让 Rocksdb 对 blob 写入进行优化。从而使得 KStore 不仅可以支持海量小文件，也可以较好的支持大文件。
让 RocksDB 支持 Blob 存储 原生的 RocksDB 其实不适合存储Blob（Value 较大），因为分层模型和日志的存在，数据放大会更明显，KV在执行后台compaction也会更慢。业界提出的解决方案主要就是“Key Value 分开存储”，这样可以独立对 Blob 文件进行优化，减少写放大和compaction影响。具体可以阅读 Wisckey 的论文（阿里云应该在13年的时候就已经在内部KV实现了这类优化，支持其上OSS类BLOB业务)。
目前，PingCAP 公司基于 RocksDB 和 Wisckey 实现了一个 RocksDB 的插件 TitanDB. 上层程序可以简单的通过 Titan::DB 的命名空间来使用。因此，我们将Titan 替换原生的 RocksDB，使得 KStore 可以较好的存储大文件。
理论上，KStore 层面的代码完全不用修改，性能也不会有损耗。相对于 FileStore，没有 Double Write 的过程，性能更好。相对于 BlueStore，实现上更简单更易维护，对小文件更友善，性能类似。具体的性能报告待整理完善。
Read Stripe Cache 导致的内存泄漏  issue地址 PR地址 与社区讨论地址  这个问题在19年5月分的时候就已经提交到社区，中间因为同事转岗以及社区对read-cache是否有意义有分歧，拖到最近才合并到主线。">
<meta itemprop="datePublished" content="2020-01-13T15:50:31&#43;08:00" />
<meta itemprop="dateModified" content="2020-01-13T15:50:31&#43;08:00" />
<meta itemprop="wordCount" content="144">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="最近在 Ceph KStore 上的一些工作（2020-01-13）"/>
<meta name="twitter:description" content="背景 Ceph 目前自带了一些单机存储引擎，包括
 基于文件的FileStore; 基于KV的KStore； 基于RocksDB&#43;裸盘的BlueStore； 基于内存的测试用MemStore； 基于seastar实现的全用户态SeaStore。  这些单机存储引擎都有适用的用户场景和读写模型。对于海量小文件的场景下，KStore 是一个很好的选择，LSM 架构的 KV 引擎天然的将数据聚合为大文件，顺序的存储到磁盘中。但是，KStore 一直以来应用较少，社区在上面维护的资源也较小。
最近，我们希望在 Rocksdb 层面进行优化，让 Rocksdb 对 blob 写入进行优化。从而使得 KStore 不仅可以支持海量小文件，也可以较好的支持大文件。
让 RocksDB 支持 Blob 存储 原生的 RocksDB 其实不适合存储Blob（Value 较大），因为分层模型和日志的存在，数据放大会更明显，KV在执行后台compaction也会更慢。业界提出的解决方案主要就是“Key Value 分开存储”，这样可以独立对 Blob 文件进行优化，减少写放大和compaction影响。具体可以阅读 Wisckey 的论文（阿里云应该在13年的时候就已经在内部KV实现了这类优化，支持其上OSS类BLOB业务)。
目前，PingCAP 公司基于 RocksDB 和 Wisckey 实现了一个 RocksDB 的插件 TitanDB. 上层程序可以简单的通过 Titan::DB 的命名空间来使用。因此，我们将Titan 替换原生的 RocksDB，使得 KStore 可以较好的存储大文件。
理论上，KStore 层面的代码完全不用修改，性能也不会有损耗。相对于 FileStore，没有 Double Write 的过程，性能更好。相对于 BlueStore，实现上更简单更易维护，对小文件更友善，性能类似。具体的性能报告待整理完善。
Read Stripe Cache 导致的内存泄漏  issue地址 PR地址 与社区讨论地址  这个问题在19年5月分的时候就已经提交到社区，中间因为同事转岗以及社区对read-cache是否有意义有分歧，拖到最近才合并到主线。"/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://liuchang0812.github.io" class="f3 fw2 hover-white no-underline white-90 dib">
      Notes
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://liuchang0812.github.io/" title="Home page">
              Home
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://github.com/liuchang0812" title="Works page">
              Works
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://liuchang0812.github.io/tags/" title="Tags page">
              Tags
            </a>
          </li>
          
        </ul>
      
      









    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        POSTS
      </p>
      <h1 class="f1 athelas mb1">最近在 Ceph KStore 上的一些工作（2020-01-13）</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2020-01-13T15:50:31&#43;08:00">January 13, 2020</time>      
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h1 id="背景">背景</h1>
<p>Ceph 目前自带了一些单机存储引擎，包括</p>
<ul>
<li>基于文件的FileStore;</li>
<li>基于KV的KStore；</li>
<li>基于RocksDB+裸盘的BlueStore；</li>
<li>基于内存的测试用MemStore；</li>
<li>基于seastar实现的全用户态SeaStore。</li>
</ul>
<p>这些单机存储引擎都有适用的用户场景和读写模型。对于海量小文件的场景下，KStore 是一个很好的选择，LSM 架构的 KV 引擎天然的将数据聚合为大文件，顺序的存储到磁盘中。但是，KStore 一直以来应用较少，社区在上面维护的资源也较小。</p>
<p>最近，我们希望在 Rocksdb 层面进行优化，让 Rocksdb 对 blob 写入进行优化。从而使得 KStore 不仅可以支持海量小文件，也可以较好的支持大文件。</p>
<h1 id="让-rocksdb-支持-blob-存储">让 RocksDB 支持 Blob 存储</h1>
<p>原生的 RocksDB 其实不适合存储Blob（Value 较大），因为分层模型和日志的存在，数据放大会更明显，KV在执行后台compaction也会更慢。业界提出的解决方案主要就是“Key Value 分开存储”，这样可以独立对 Blob 文件进行优化，减少写放大和compaction影响。具体可以阅读 <a href="https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf">Wisckey</a> 的论文（阿里云应该在13年的时候就已经在内部KV实现了这类优化，支持其上OSS类BLOB业务)。</p>
<p>目前，PingCAP 公司基于 RocksDB 和 Wisckey 实现了一个 RocksDB 的插件 <a href="https://github.com/tikv/titan">TitanDB</a>. 上层程序可以简单的通过 Titan::DB 的命名空间来使用。因此，我们将Titan 替换原生的 RocksDB，使得 KStore 可以较好的存储大文件。</p>
<p>理论上，KStore 层面的代码完全不用修改，性能也不会有损耗。相对于 FileStore，没有 Double Write 的过程，性能更好。相对于 BlueStore，实现上更简单更易维护，对小文件更友善，性能类似。具体的性能报告待整理完善。</p>
<h1 id="read-stripe-cache-导致的内存泄漏">Read Stripe Cache 导致的内存泄漏</h1>
<ul>
<li><a href="https://tracker.ceph.com/issues/39665">issue地址</a></li>
<li><a href="https://github.com/ceph/ceph/pull/32538">PR地址</a></li>
<li><a href="https://github.com/ceph/ceph/pull/28056">与社区讨论地址</a></li>
</ul>
<p>这个问题在19年5月分的时候就已经提交到社区，中间因为同事转岗以及社区对read-cache是否有意义有分歧，拖到最近才合并到主线。</p>
<p>简单的讲，问题是KStore会为一个 transaction 的写入读取缓存 stripe，来保护一个事务中的写入只用更新内存。这块Cache会在事务结束的时候释放。但是在读文件的时候也有类似的Cache，但是读操作并不通过事务提交，这块儿内存没有释放。本质上读类操作并不需要 stripe cache，经过来来回回的讨论，大家终于达成一致，彻底在 read 操作中移除 stripe cache。</p>
<h1 id="onode-cache-导致的内存泄漏">Onode Cache 导致的内存泄漏</h1>
<ul>
<li><a href="https://tracker.ceph.com/issues/43436">issue地址</a></li>
<li><a href="https://github.com/ceph/ceph/pull/32446">PR地址</a></li>
</ul>
<p>在KStore的实现中，每打开一个 RADOS obj都会在内存中缓存 onode 结构体。在长时间的测量中发现，内存占用会缓慢上涨，直到 OOM。通过代码阅读，怀疑是 Onode Cache 没有释放。直接 GDB 线上进程，可以发现每个 collection 缓存了 9k+ 的 Onode, 当前进程共打开了399个 collection。通过计算，这一块就占用了1.7GB的内存。</p>
<p>为了更好的观察 onode 的内存占用，我们添加了 mempool 对 KStore 的支持，可以直接通过 asock 命令来查看实时的 onode 个数与内存占用，对应的代码为<a href="https://github.com/ceph/ceph/pull/32446/commits/c3811eaba9444ee68612a88f900e70fb7948c5c1">链接</a>。</p>
<p>查看了 BlueStore 的实现，会有一个后台线程 MempoolThread 周期的释放 Onode Cache，同时也会在读写操作时，根据 Cache 来释放一些 Onode Cache。目前，我们简单的添加了一个后台进程来周期的释放 Onode Cache，将来可以基于 mempool 来做更精细化的内存管控。</p>
<h1 id="pending-stripes-并发访问的-segfault-core">Pending Stripes 并发访问的 SegFault Core</h1>
<ul>
<li><a href="http://tracker.ceph.com/issues/43520">issue地址</a></li>
<li><a href="https://github.com/ceph/ceph/pull/32540">PR地址</a></li>
</ul>
<p>这个问题比较简单，在测试的时候遇到 OSD Core，通过 GDB 怀疑和 pending stripes 结构体的并发访问有关。加锁验证了一下，确实不会再出现 SegFault。于是修改为性能更好的读写锁。</p>
<ul class="pa0">
  
</ul>
<div class="mt6">
        
      </div>
    </section>

    <aside class="w-30-l mt6-l"><div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">What's in this Post</p>
      <nav id="TableOfContents"></nav>
  </div>




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://liuchang0812.github.io" >
    &copy; 2020 Notes
  </a>
    <div>








</div>
  </div>
</footer>

    

  <script src="https://liuchang0812.github.io/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
